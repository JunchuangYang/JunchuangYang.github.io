<html>
  <head>
    <title>Scrapy框架中Pipeline用法 - Y&#39;s Blog</title>
    <link href='/images/fav.png' rel='shortcut icon'>
<link href='/atom.xml' rel='alternate' type='application/rss+xml'>
<link rel="stylesheet" href="/css/style.css">
<link rel="stylesheet" href="/css/highlight.css">
<link rel="stylesheet" href="/css/responsive.css">
<script src="/js/jquery.js"></script>
<script src="/js/basics.js"></script>
<meta content='width=device-width, initial-scale=1.0, user-scalable=no' name='viewport'>
<meta content='text/html; charset=utf-8' http-equiv='content-type'>


  </head>
  <body>
    <header>
  <a id='go-back-home' href='/'><img src='/images/scribble.png' alt='Home' width='53' height='59'></a>
  <p>Y&#39;s Blog</p>
  <p>Salted fish without dreams</p>
</header>

    <div id='container'>
      <div class='block'>
  
    <a class='main' href='/'>Home</a>
  
    <a class='main' href='/Notes'>Notes</a>
  
    <a class='main' href='/about'>About</a>
  
    <a class='main' href='https://github.com/JunchuangYang'>Github</a>
  
</div>

      <section class='paging'>
  
    <div class='left'>
      <a href='/2018/10/30/Scrapy之Spider/'>
        ‹
      </a>
    </div>
  
  
    <div class='right'>
      <a href='/2018/10/28/Scrapy命令行详解/'>
        ›
      </a>
    </div>
  
</section>

      <div class='content'>
        <section class='post'>
          <h1>
            <div class='date'>2018-10-30</div>
            Scrapy框架中Pipeline用法
          </h1>
          <p>当Item 在Spider中被收集之后，就会被传递到Item Pipeline中进行处理</p>
<p>每个item pipeline组件是实现了简单的方法的python类，负责接收到item并通过它执行一些行为，同时也决定此Item是否继续通过pipeline,或者被丢弃而不再进行处理</p>
<p>item pipeline的主要作用：</p>
<ol>
<li>清理html数据</li>
<li>验证爬取的数据</li>
<li>去重并丢弃</li>
<li>讲爬取的结果保存到数据库中或文件中</li>
</ol>
<p><strong>自己编写pipeline</strong></p>
<p><strong>process_item(self,item,spider)</strong></p>
<p>每个item piple组件是一个独立的pyhton类，<strong>必须实现以process_item(self,item,spider)方法</strong></p>
<p>每个item pipeline组件都需要调用该方法，这个方法必须返回一个具有数据的dict,或者item对象，或者抛出DropItem异常，被丢弃的item将不会被之后的pipeline组件所处理</p>
<p>下面的方法也可以选择实现</p>
<p><strong>open_spider(self,spider)</strong>：表示当spider被开启的时候调用这个方法</p>
<p><strong>close_spider(self,spider)</strong>:spider结束的时候这个方法被调用</p>
<p><strong>from_crawler(cls,crawler)</strong>:这个和我们在前面说spider的时候的用法是一样的，可以用于获取settings配置文件中的信息</p>
<p>自己编写了TextPipeline和MongoPipeLine</p>
<p>TextPipeline：用来出来返回的Item，对Item[‘text’]进行长度限制</p>
<p>MongoPipeline：将数据插入到MongoDB中，同时，也演示了from_crawler如何从settings中回去配置数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define your item pipelines here</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Don't forget to add your pipeline to the ITEM_PIPELINES setting</span></span><br><span class="line"><span class="comment"># See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pymongo</span><br><span class="line"><span class="keyword">from</span> scrapy.exceptions <span class="keyword">import</span> DropItem</span><br><span class="line"></span><br><span class="line"><span class="comment"># item 的处理</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextPipeline</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span>  <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.limit=<span class="number">50</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self, item, spider)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> item[<span class="string">'text'</span>]:</span><br><span class="line">            <span class="keyword">if</span> len(item[<span class="string">'text'</span>])&gt;self.limit:</span><br><span class="line">                item[<span class="string">'text'</span>] = item[<span class="string">'text'</span>][<span class="number">0</span>:self.limit].rstrip()+<span class="string">'...'</span></span><br><span class="line">                <span class="keyword">return</span>  item[<span class="string">'text'</span>]</span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            <span class="keyword">return</span> DropItem(<span class="string">'Missing Text'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存到MongoDB</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MongoPipeLine</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self,mongo_uri,mongo_db)</span>:</span></span><br><span class="line">        self.mongo_uri = mongo_uri</span><br><span class="line">        self.mongo_db =mongo_db</span><br><span class="line"></span><br><span class="line"><span class="meta">    @classmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">from_crawler</span><span class="params">(cls,crawler)</span>:</span></span><br><span class="line">        <span class="keyword">return</span>  cls(</span><br><span class="line">            mongo_uri = crawler.setting.get(<span class="string">'MONGO_URI'</span>),</span><br><span class="line">            mongo_db = crawler.setting.get(<span class="string">'MONGO_DB'</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 爬虫启动时进行的相关操作</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">open_spider</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">        self.client = pymongo.MongoClient(self.mongo_uri)</span><br><span class="line">        self.db = self.client[self.mongo_db]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># item插入到MongoDB</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process_item</span><span class="params">(self,item,spider)</span>:</span></span><br><span class="line">        name = item.__class__.__name__</span><br><span class="line">        self.db[name].insert(dict(item))</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">close_spider</span><span class="params">(self,spider)</span>:</span></span><br><span class="line">        self.client.close()</span><br></pre></td></tr></table></figure>
<p><strong>启用一个item Pipeline组件</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Configure item pipelines</span></span><br><span class="line"><span class="comment"># See https://doc.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line">ITEM_PIPELINES = &#123;</span><br><span class="line">   <span class="string">'quote.pipelines.TextPipeline'</span>: <span class="number">300</span>,</span><br><span class="line">   <span class="string">'quote.pipelines.MongoPipeline'</span>: <span class="number">400</span>,</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>每个pipeline后面有一个数值，这个数组的范围是0-1000，这个数值确定了他们的运行顺序，数字越小优先级遇高</p>

          <br>
<p>Y&#39;s Blog</p>
<p><img src='/images/scribble3.png' alt='scribble'></p>

        </section>
      </div>
      
      <div class='block'>
  
    <a class='main' href='/'>Home</a>
  
    <a class='main' href='/Notes'>Notes</a>
  
    <a class='main' href='/about'>About</a>
  
    <a class='main' href='https://github.com/JunchuangYang'>Github</a>
  
</div>

    </div>
    <footer>
  <span class='muted'>&copy; Junchuang Yang. All Rights Reserved.</span><br>
  <a href='https://github.com/saintwinkle/hexo-theme-scribble' class='muted'>built with Hexo using Scribble theme</a>
  <br>
   <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  <span id="busuanzi_container_site_pv">
    本站访问量<span id="busuanzi_value_site_pv"></span>次
  </span>
  <br>
  <img src='/images/scribble2.png' alt='scribble' />
</footer>

  </body>
</html>
